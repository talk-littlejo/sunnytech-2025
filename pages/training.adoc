= 🧗Entraînement de l'agent
:imagesdir: assets/default/images
image::mi-training.png[]
//mi-2
[NOTE.speaker]
====
Maintenant que la mission est bien définie, il est temps de passer à l’entraînement.

Pour ça, on va se mettre en condition réelle… mais à petite échelle.
====

== Terrain d’exercice : KinD

* Un seul serveur
* Docker

image::kind-logo.png[width=45%]

[NOTE.speaker]
====
On utilise KinD : Kubernetes IN Docker.
Distribution légère, parfaite pour simuler des clusters sur un seul serveur.
Et surtout : ça tourne bien dans des environnements automatisés comme la CI/CD.
Justement, on va voir que l'on l’a intégré dans notre pipeline.
====

== CI/CD : GitHub Action

image::github-action.png[]

[NOTE.speaker]
====
Tester à la main serait trop lent.

On a tout automatisé avec GitHub Actions.

Pourquoi ?

* "Gratuit"
* VM temporaires avec 4 CPU pour simuler des opérations

=> déploiement possible jusqu'à 14 clusters

Mais lancer les tests automatiquement, c’est une chose.
Encore faut-il pouvoir créer, connecter et détruire toute l’infra à la volée.
Et pour ça…

====

== Infrastructure as code

image::iac.apng[]

[NOTE.speaker]
====
…on a besoin d’un outil pour piloter l’infrastructure elle-même.

C’est le rôle de l’Infrastructure as Code.

* Terraform / Opentofu
* Pulumi
* Crossplane

====

== Pulumi

image::pulumi.gif[width=70%]

[NOTE.speaker]
====
🕶️ Décision : Pulumi

Pourquoi ?

* Souplesse car c'est du code orienté développement
* Apprendre autre chose que terraform : comparatif
====

== Pulumi

[source,python,linenums]
----
kind = local.Command("kindCluster",
    create="kind create cluster --config kind.yaml --name cmesh1"
)

kind2 = local.Command("kindCluster2",
    create="kind create cluster --config kind-2.yaml --name cmesh2"
)

cmesh1_provider = cilium.Provider("cmesh1", context="kind-cmesh1", opts=pulumi.ResourceOptions(depends_on=[kind]))
cmesh2_provider = cilium.Provider("cmesh2", context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[kind2]))

cmesh1_cilium = cilium.Install("cmesh1Install",
    sets=[
        "cluster.name=cmesh1",
        "cluster.id=1",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind], providers=[cmesh1_provider]),
)

cmesh2_cilium = cilium.Install("cmesh2Install",
    sets=[
        "cluster.name=cmesh2",
        "cluster.id=2",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind2], providers=[cmesh2_provider]),
)

cmesh1_cmeshenable = cilium.Clustermesh("cmesh1Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cilium], providers=[cmesh1_provider]))
cmesh2_cmeshenable = cilium.Clustermesh("cmesh2Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh2_cilium], providers=[cmesh2_provider]))

cilium.ClustermeshConnection("cmeshConnect", destination_context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cmeshenable], providers=[cmesh1_provider]))
----


[NOTE.speaker]
====
* langage de programmation

🕶️ Décision : Python

Maintenant qu’on maîtrise les bases, une question se pose… Et si on poussait un peu plus loin ? Et si on arrivait à créer 511 clusters ?

====

== Tester les limites de KinD
image::15-clusters.apng[width=50%]

[NOTE.speaker]
====
🎛️ Matériel utilisé :

* 🖥️ 16 CPU — 🧠 32 Go de RAM

🚫 Résultat :

* Blocage à 15 clusters maximum contre 14 pour GitHub Action
* Temps de déploiement : 45 minutes

====
