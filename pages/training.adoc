= ğŸ§—EntraÃ®nement de l'agent
:imagesdir: assets/default/images
image::mi-training.png[]
//mi-2
[NOTE.speaker]
====
Maintenant que la mission est bien dÃ©finie, il est temps de passer Ã  lâ€™entraÃ®nement.

Pour Ã§a, on va se mettre en condition rÃ©elleâ€¦ mais Ã  petite Ã©chelle.
====

== Terrain dâ€™exercice : KinD

* Un seul serveur
* Docker

image::kind-logo.png[width=45%]

[NOTE.speaker]
====
On utilise KinD : Kubernetes IN Docker.
Distribution lÃ©gÃ¨re, parfaite pour simuler des clusters sur un seul serveur.
Et surtout : Ã§a tourne bien dans des environnements automatisÃ©s comme la CI/CD.
Justement, on va voir que l'on lâ€™a intÃ©grÃ© dans notre pipeline.
====

== CI/CD : GitHub Action

image::github-action.png[]

[NOTE.speaker]
====
Tester Ã  la main serait trop lent.

On a tout automatisÃ© avec GitHub Actions.

Pourquoi ?

* "Gratuit"
* VM temporaires avec 4 CPU pour simuler des opÃ©rations

=> dÃ©ploiement possible jusqu'Ã  14 clusters

Mais lancer les tests automatiquement, câ€™est une chose.
Encore faut-il pouvoir crÃ©er, connecter et dÃ©truire toute lâ€™infra Ã  la volÃ©e.
Et pour Ã§aâ€¦

====

== Infrastructure as code

image::iac.apng[]

[NOTE.speaker]
====
â€¦on a besoin dâ€™un outil pour piloter lâ€™infrastructure elle-mÃªme.

Câ€™est le rÃ´le de lâ€™Infrastructure as Code.

* Terraform / Opentofu
* Pulumi
* Crossplane

====

== Pulumi

image::pulumi.gif[width=70%]

[NOTE.speaker]
====
ğŸ•¶ï¸ DÃ©cision : Pulumi

Pourquoi ?

* Souplesse car c'est du code orientÃ© dÃ©veloppement
* Apprendre autre chose que terraform : comparatif
====

== Pulumi

[source,python,linenums]
----
kind = local.Command("kindCluster",
    create="kind create cluster --config kind.yaml --name cmesh1"
)

kind2 = local.Command("kindCluster2",
    create="kind create cluster --config kind-2.yaml --name cmesh2"
)

cmesh1_provider = cilium.Provider("cmesh1", context="kind-cmesh1", opts=pulumi.ResourceOptions(depends_on=[kind]))
cmesh2_provider = cilium.Provider("cmesh2", context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[kind2]))

cmesh1_cilium = cilium.Install("cmesh1Install",
    sets=[
        "cluster.name=cmesh1",
        "cluster.id=1",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind], providers=[cmesh1_provider]),
)

cmesh2_cilium = cilium.Install("cmesh2Install",
    sets=[
        "cluster.name=cmesh2",
        "cluster.id=2",
        "ipam.mode=kubernetes",
    ],
    version="1.15.5",
    opts=pulumi.ResourceOptions(depends_on=[kind2], providers=[cmesh2_provider]),
)

cmesh1_cmeshenable = cilium.Clustermesh("cmesh1Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cilium], providers=[cmesh1_provider]))
cmesh2_cmeshenable = cilium.Clustermesh("cmesh2Enable", service_type="NodePort", opts=pulumi.ResourceOptions(depends_on=[cmesh2_cilium], providers=[cmesh2_provider]))

cilium.ClustermeshConnection("cmeshConnect", destination_context="kind-cmesh2", opts=pulumi.ResourceOptions(depends_on=[cmesh1_cmeshenable], providers=[cmesh1_provider]))
----


[NOTE.speaker]
====
* langage de programmation

ğŸ•¶ï¸ DÃ©cision : Python

Maintenant quâ€™on maÃ®trise les bases, une question se poseâ€¦ Et si on poussait un peu plus loin ? Et si on arrivait Ã  crÃ©er 511 clusters ?

====

== Tester les limites de KinD
image::15-clusters.apng[width=50%]

[NOTE.speaker]
====
ğŸ›ï¸ MatÃ©riel utilisÃ© :

* ğŸ–¥ï¸ 16 CPU â€” ğŸ§  32 Go de RAM

ğŸš« RÃ©sultat :

* Blocage Ã  15 clusters maximum contre 14 pour GitHub Action
* Temps de dÃ©ploiement : 45 minutes

====
