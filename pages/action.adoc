= PrÃ©sentation du terrain d'action
:imagesdir: assets/default/images
image::mi-action.png[]
//mi-4
[NOTE.speaker]
====
Maintenant que notre environnement d'entraÃ®nement est en place, Il est temps maintenant de **passer Ã  lâ€™action**.
====

== Choix stratÃ©gique : AWS EKS

[NOTE.speaker]
====
ğŸ§­ Premier choix dÃ©cisif : **le terrain dâ€™opÃ©ration**.  
Nous avons besoin dâ€™une plateforme capable dâ€™encaisser 511 clusters sans flancher.

ğŸ¯ Cible retenue : **AWS EKS**

* âœ… maÃ®trise d'AWS et d'EKS
* ğŸ›¡ï¸ Confiance pour supporter 511 clusters
====

== Limites imposÃ©es par AWS

image::service-quota.png[]

[NOTE.speaker]
====
ğŸ’¥ Premier obstacle : les limites AWS

AWS bloque Ã  **100 clusters max**.

ğŸ© Tentative : nÃ©gociation.

â³ AprÃ¨s plusieurs semaines : refus pour 511.

âœ… RÃ©sultat obtenu : **256 clusters** autorisÃ©s.

Mais pour les crÃ©er dans les temps, il va falloir ruserâ€¦
====

== Bonnes pratiques vs RÃ©alitÃ© opÃ©rationnelle

image::nodepool.apng[width=60%]

[NOTE.speaker]
====
Prenons un exemple.

ğŸ“˜ Selon les bonnes pratiques AWS, pour crÃ©er des workers EKS, il faut : crÃ©er des Node Pools, qui crÃ©ent des ASG  qui crÃ©ent des EC2.

ğŸš¨ Sauf que tout Ã§a prend un **temps fou**.

ğŸ¯ On va crÃ©er **directement des EC2**, sans passer par les surcouches d'AWS.

ğŸ‘‰ Allez, regardons maintenant lâ€™architecture retenue pour aller jusquâ€™Ã  256 clusters.
====

== Architecture retenue

image::aws-archi.svg[width=50%]

[NOTE.speaker]
====
On a un compte AWS, un VPC, un rÃ©seau privÃ©. Ã€ l'intÃ©rieur il contient 4 sous-rÃ©seaux 2 publics 2 privÃ©s.

Dans les sous-rÃ©seaux public il y a une NAT Gateway pour pouvoir tÃ©lÃ©charger les images des containers depuis les sous-rÃ©seaux privÃ©s.

Dans les rÃ©seaux privÃ©, il y a les clusters EKS avec une seule EC2 et un control plane.

On avait Ã©voquÃ© le problÃ¨me des connexions Cilium â€” il est temps de voir comment on lâ€™a attaquÃ©.
====

== ParallÃ©lisation des connexions

image::connection-answer.apng[width=45%]
[NOTE.speaker]
====
Je vous prÃ©sente la premiÃ¨re tentative pour parallÃ©liser les connexions.

La contrainte : pas de crÃ©ation des connexions d'un mÃªme cluster en parallÃ¨le

Ainsi avec cet algorithme, avec 6 clusters kubernetes on a 5 Ã©tapes.

On passe donc d'une complexitÃ© de O(n2) Ã  O(n).

On va maintenant pouvoir tester cette parallÃ©lisation sur 32 clusters Kubernetes.
====

== 32 clusters

image::mission_failed.apng[width=50%]

[NOTE.speaker]
====
Le test de 32 clusters a Ã©chouÃ©

âŒ Mur technique dÃ©tectÃ© :

* ğŸ“¦ Trop dâ€™objets Pulumi
* n x n-1 / 2 objets pour crÃ©er les connexion
* explosion de la RAM ğŸ’¥

Je vais rÃ©duire Ã  16 clusters
====

== 16 clusters

image::16-clusters.apng[width=50%]

[NOTE.speaker]
====
ğŸ“‰ RÃ©sultat :

* âœ… 16 clusters connectÃ©s
* â±ï¸ 45 minutesâ€¦
* ğŸš« Bien trop long pour 511 clusters

ğŸ” Connexions entre clusters â†’ explosion du CPU

* 1 connexion â‰ˆ 1 CPU utilisÃ©
* 128 connexions = 128 CPUs ? ğŸ˜…


* Il faut une autre stratÃ©gie de connexion pour aller plus loin.
====
